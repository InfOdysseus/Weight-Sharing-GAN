{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import itertools\n",
    "import math\n",
    "import time\n",
    "from MiddleBlock.DiscriminatorMiddleBlock import DiscriminatorMiddleBlock\n",
    "from MiddleBlock.GeneratorMiddleBlock import GeneratorMiddleBlock\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.transforms as transforms\n",
    "from IPython import display\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.models as models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = 100 # 노이즈 벡터의 크기\n",
    "nc = 1 # 채널의 수\n",
    "ngf = 64 # generator 필터 조정\n",
    "ndf = 64 # discriminator 필터 조정\n",
    "niter = 200 # 에폭 수\n",
    "lr = 0.0001\n",
    "beta1 = 0.9\n",
    "\n",
    "imageSize = 64 # 만들어지는 이미지의 크기\n",
    "batchSize = 64 # 미니배치의 크기\n",
    "outf = \"result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\raw\\train-images-idx3-ubyte.gz to ./data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%C:\\Users\\roybatty0601\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data/MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize(64),\n",
    "        transforms.ToTensor()                    \n",
    "])\n",
    "\n",
    "dataset = dsets.MNIST(root='./data/', train=True, download=True, transform=transform)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [50000, 10000])\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size= batchSize, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size= batchSize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:         # Conv weight init\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:  # BatchNorm weight init\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _netG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netG, self).__init__()\n",
    "        self.firstConv = nn.Conv2d(ndf, 10+1, 3)\n",
    "        self.randomVec = nn.ConvTranspose2d(nz, ngf, 3, 1, 0)\n",
    "        self.catConv = nn.ConvTranspose2d(ngf+ngf, ngf, 3, 1, 0)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(ngf, nc, 3, padding = 1)\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            GeneratorMiddleBlock((ngf, ngf, 3, 1, 0),(ndf, ngf, 3, 1, 0)),\n",
    "            GeneratorMiddleBlock((ngf, ngf, 3, 1, 0),(ndf, ngf, 3, 1, 0), batchNorm = True),\n",
    "            GeneratorMiddleBlock((ngf, ngf, 3, 1, 0),(ndf, ngf, 3, 2, 0)),\n",
    "            GeneratorMiddleBlock((ngf, ngf, 3, 2, 0),(ndf, ngf, 3, 1, 0), batchNorm = True),\n",
    "            GeneratorMiddleBlock((ngf, ngf, 3, 1, 0),(ndf, ngf, 3, 2, 0)),\n",
    "            GeneratorMiddleBlock((ngf, ngf, 3, 2, 0),(ndf, ngf, 3, 1, 0), batchNorm = True),\n",
    "            GeneratorMiddleBlock((ngf, ngf, 3, 1, 0),(ndf, ngf, 3, 1, 0), batchNorm = True),\n",
    "            GeneratorMiddleBlock((ngf, ngf, 3, 1, 0),(ndf, ngf, 3, 1, 0)),\n",
    "            GeneratorMiddleBlock((ngf, ngf, 3, 1, 0),(nc, ngf, 4, 1, 0)),\n",
    "            GeneratorMiddleBlock((nc, ngf, 4, 1, 0), last=True, img_channel = 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input, label):\n",
    "        first = self.firstConv.weight[label]\n",
    "        \n",
    "        output = torch.cat([F.relu(self.randomVec(input)), first], dim=1)\n",
    "        output = self.catConv(output)\n",
    "        output = self.main(output)\n",
    "        output = torch.tanh(output)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _netD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD, self).__init__()\n",
    "        self.labelClassify = nn.Conv2d(ndf, 10+1, 3, 1, 0)\n",
    "        self.main = nn.Sequential(\n",
    "            # (nc) x 64 x 64)\n",
    "            DiscriminatorMiddleBlock(nc, ndf, 4, dropout_ratio=0.5),\n",
    "            DiscriminatorMiddleBlock(ndf, ndf, 3, dropout_ratio=0.5),\n",
    "            DiscriminatorMiddleBlock(ndf, ndf, 3, dropout_ratio=0.5, batchNorm = True),\n",
    "            DiscriminatorMiddleBlock(ndf, ndf, 3, dropout_ratio=0.5, batchNorm = True), #55\n",
    "            DiscriminatorMiddleBlock(ndf, ndf, 3, 2, dropout_ratio=0.5), #27\n",
    "            DiscriminatorMiddleBlock(ndf, ndf, 3, dropout_ratio=0.5, batchNorm = True), #25\n",
    "            DiscriminatorMiddleBlock(ndf, ndf, 3, 2, dropout_ratio=0.5), #23\n",
    "            DiscriminatorMiddleBlock(ndf, ndf, 3, dropout_ratio=0.5, batchNorm = True), #11\n",
    "            DiscriminatorMiddleBlock(ndf, ndf, 3, dropout_ratio=0.5), #5\n",
    "            DiscriminatorMiddleBlock(ndf, ndf, 3), #3\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return self.labelClassify(output).view(output.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netG(\n",
      "  (firstConv): Conv2d(64, 11, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (randomVec): ConvTranspose2d(100, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (catConv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (main): Sequential(\n",
      "    (0): GeneratorMiddleBlock(\n",
      "      (branch): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (firstTransposed): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (secondTransposed): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "    (1): GeneratorMiddleBlock(\n",
      "      (branch): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (firstTransposed): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (secondTransposed): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "    (2): GeneratorMiddleBlock(\n",
      "      (branch): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (firstTransposed): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (secondTransposed): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    )\n",
      "    (3): GeneratorMiddleBlock(\n",
      "      (branch): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (firstTransposed): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (secondTransposed): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "    (4): GeneratorMiddleBlock(\n",
      "      (branch): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (firstTransposed): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (secondTransposed): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    )\n",
      "    (5): GeneratorMiddleBlock(\n",
      "      (branch): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (firstTransposed): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (secondTransposed): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "    (6): GeneratorMiddleBlock(\n",
      "      (branch): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (firstTransposed): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (secondTransposed): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "    (7): GeneratorMiddleBlock(\n",
      "      (branch): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (firstTransposed): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (secondTransposed): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "    (8): GeneratorMiddleBlock(\n",
      "      (branch): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (firstTransposed): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (secondTransposed): ConvTranspose2d(128, 1, kernel_size=(4, 4), stride=(1, 1))\n",
      "    )\n",
      "    (9): GeneratorMiddleBlock(\n",
      "      (branch): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv): Conv2d(1, 64, kernel_size=(4, 4), stride=(1, 1))\n",
      "      (firstTransposed): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "_netD(\n",
      "  (labelClassify): Conv2d(64, 11, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (main): Sequential(\n",
      "    (0): DiscriminatorMiddleBlock(\n",
      "      (conv): Conv2d(1, 64, kernel_size=(4, 4), stride=(1, 1))\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (1): DiscriminatorMiddleBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (2): DiscriminatorMiddleBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (3): DiscriminatorMiddleBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (4): DiscriminatorMiddleBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (5): DiscriminatorMiddleBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (6): DiscriminatorMiddleBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (7): DiscriminatorMiddleBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (8): DiscriminatorMiddleBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (9): DiscriminatorMiddleBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def copyWeight(netD, netG, tau):\n",
    "    netG.firstConv.load_state_dict(netD.labelClassify.state_dict())\n",
    "    for i, layer in enumerate(netG.main):\n",
    "        layer.conv.load_state_dict(netD.main[len(netD.main) - i - 1].conv.state_dict())\n",
    "        \n",
    "netG = _netG().to(device)\n",
    "netG.apply(weights_init)\n",
    "print(netG)\n",
    "\n",
    "netD = _netD().to(device)\n",
    "netD.apply(weights_init)\n",
    "print(netD)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "copyWeight(netD, netG, 0.05)\n",
    "fixed_noise = torch.randn(batchSize, nz, 1, 1, device=device)\n",
    "fixed_noise_label = torch.randint(10, size = (batchSize,), device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerD = optim.RMSprop(netD.parameters(), lr=lr)\n",
    "optimizerG = optim.RMSprop(netG.parameters(), lr=lr)\n",
    "writer = SummaryWriter()\n",
    "total_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 step\n",
      "199 step\n",
      "299 step\n",
      "399 step\n",
      "499 step\n",
      "599 step\n",
      "699 step\n",
      "99 step\n",
      "199 step\n",
      "299 step\n",
      "399 step\n",
      "499 step\n",
      "599 step\n",
      "699 step\n",
      "99 step\n",
      "199 step\n",
      "299 step\n",
      "399 step\n",
      "499 step\n",
      "599 step\n",
      "699 step\n",
      "99 step\n",
      "199 step\n",
      "299 step\n",
      "399 step\n",
      "499 step\n",
      "599 step\n",
      "699 step\n",
      "99 step\n",
      "199 step\n",
      "299 step\n",
      "399 step\n",
      "499 step\n",
      "599 step\n",
      "699 step\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    data = None\n",
    "    #netG.train()\n",
    "    for i, (data,label) in enumerate(train_loader):\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        batch_size = data.shape[0]\n",
    "        label = label.to(device)\n",
    "        output= netD(data.to(device))\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        \n",
    "        # train with fake\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        \n",
    "        output = netD(netG(noise, label).detach())\n",
    "        fake_labels = torch.ones_like(label)*10\n",
    "        errD_fake = criterion(output, fake_labels)\n",
    "        errD_fake.backward()\n",
    "        errD = errD_real + errD_fake\n",
    "        writer.add_scalar('Discriminator total loss',\n",
    "                                      errD, total_step)\n",
    "        optimizerD.step()\n",
    "        copyWeight(netD, netG,0.05)\n",
    "        \n",
    "        netG.zero_grad()\n",
    "        \n",
    "        fake = netG(noise, label)\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        writer.add_scalar('Generator total loss',\n",
    "                                      errG, total_step)\n",
    "\n",
    "        \n",
    "        #sampleNoise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        #sampleLoss = F.smooth_l1_loss(netG(sampleNoise,randomLabel).detach(),netG(noise, randomLabel))\n",
    "        #(-torch.log(sampleLoss)).backward()\n",
    "        optimizerG.step()\n",
    "        total_step += 1\n",
    "        if ((i+1) % 100 == 0):\n",
    "            print(i, \"step\")\n",
    "            #print(sampleLoss)\n",
    "            #netG.eval()\n",
    "            fake = netG(fixed_noise, fixed_noise_label)\n",
    "            #netG.train()\n",
    "            vutils.save_image(fake.data,\n",
    "                '%s/fake_samples_epoch_%s.png' % (outf, str(epoch)+\" \"+str(i+1)),\n",
    "                normalize=True)\n",
    "    vutils.save_image(data,\n",
    "            '%s/real_samples.png' % outf,\n",
    "            normalize=True)\n",
    "    fake = netG(fixed_noise,fixed_noise_label) \n",
    "    vutils.save_image(fake.data,\n",
    "            '%s/fake_samples_epoch_%s.png' % (outf, epoch),\n",
    "            normalize=True)\n",
    "\n",
    "    # do checkpointing\n",
    "    torch.save(netG.state_dict(), '%s/netG.pth' % (outf))\n",
    "    torch.save(netD.state_dict(), '%s/netD.pth' % (outf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG.eval()\n",
    "\n",
    "randomLabel = torch.randint(10, size = (batchSize,), device=device) \n",
    "randomLabel.fill_(np.random.randint(10))\n",
    "fake = netG(fixed_noise, randomLabel)\n",
    "netG.train()\n",
    "vutils.save_image(fake.data,\n",
    "                '%s/test_%s.png' % (outf, str(epoch)+\" \"+str(i+1)),\n",
    "                normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
